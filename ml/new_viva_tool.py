from dotenv import load_dotenv
load_dotenv()
import os
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.vectorstores import FAISS
from PyPDF2 import PdfReader
from langchain.text_splitter import CharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain

os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY")

def get_pdf_text(pdf_docs):
    text = " "
    for pdf in pdf_docs:
        reader = PdfReader(pdf)
        for page in reader.pages:
            text += page.extract_text()
    return text



def get_text_chunks(text):
    text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    chunks = text_splitter.split_text(text)
    return chunks



def get_vectorstore(text_chunks):
    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)
    return vectorstore

#Approach 2


llm = ChatOpenAI(temperature=0)
prompt = ChatPromptTemplate.from_template("""
Retreive all the data from the pdfs and answer strictly according to the data in the pdfs.
You are an expert at creating viva questions for students based on the content of the pdfs.
Create 5 viva questions based on important concepts of the input pdfs and user's difficulty level which is given as input
Answer should be given in the form of a list of questions and answers.


<context>
{context}
</context>

Question: Give me viva questions on  difficulty level-{input} based on the content of the pdfs.""")

document_chain = create_stuff_documents_chain(llm, prompt)
raw_text = get_pdf_text(["history.pdf"])
text_chunks= get_text_chunks(raw_text)
vectorstore= get_vectorstore(text_chunks)
retriever = vectorstore.as_retriever()
retrieval_chain = create_retrieval_chain(retriever, document_chain)

#example input-to be replaced with a summary generated by the llm of the user profile

difficulty_level = "easy"
response = retrieval_chain.invoke({"input": "{difficulty_level}"})
print(response["answer"])
# import json
# def call_llm(difficulty_level):
#     response = retrieval_chain.invoke({"input": "{difficulty_level}"})
#     print(response["answer"])
#     data_dict = json.loads(response["answer"])

#     # Initialize the new dictionary structure
#     qa_dict = {}

#     # Iterate over the keys and values to restructure the dictionary
#     for i in range(1, 6):
#         qa_dict[i] = {
#             "question": data_dict[f"Question {i}"],
#             "answer": data_dict[f"Answer {i}"]
#         }
#     return qa_dict


# difficulty_level = "easy"


